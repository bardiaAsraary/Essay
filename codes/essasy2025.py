# -*- coding: utf-8 -*-
"""Essasy2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WV8o7yL3QA85RPHCEHGrmtm-iy951r5i
"""

# 1. Uninstall any existing versions
!pip uninstall -y fsspec gcsfs datasets

# 2. Reinstall matching versions:
#    - datasets 3.6.0 requires fsspec ‚â§2025.3.0
#    - gcsfs 2025.3.2 requires fsspec==2025.3.2
# We‚Äôll pick fsspec=2025.3.0 and install gcsfs of the same series.
!pip install \
    fsspec==2025.3.0 \
    gcsfs==2025.3.0 \
    datasets==3.6.0 \
    transformers \
    accelerate \
    --quiet

!pip install --upgrade transformers --quiet

# Install the CLI if you haven't already
!pip install huggingface_hub --quiet

# Log in (paste your token when prompted)
from huggingface_hub import login
login()

from datasets import load_dataset
dataset = load_dataset(
    "bigcode/the-stack",
    data_dir="data/python",
    split="train",
    streaming=True
)

import json

# Prepare samples in {prompt: ..., completion: ...} format
processed = []
max_samples = 5000  # limit for this example ‚Äî increase this if needed

for i, example in enumerate(dataset):
    code = example["content"]
    if code and len(code) > 100 and "def" in code:  # keep longer code with functions
        processed.append({
            "prompt": "### Python code snippet",
            "completion": "\n" + code.strip()
        })
    if i >= max_samples:
        break

# Save to a JSONL file (one sample per line)
with open("/content/python_subset.jsonl", "w") as f:
    for item in processed:
        f.write(json.dumps(item) + "\n")

print(f"‚úÖ Saved {len(processed)} examples to /content/python_subset.jsonl")

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64,expandable_segments:True"

!pip install --upgrade gcsfs fsspec
!pip install -q torch transformers datasets

# === 0. Force CPU-only (use system RAM) ===
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""   # hide GPU

# === 1. Imports ===
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback,
)
from datasets import Dataset
import torch
import json
import math

from datasets import load_dataset
import json

from huggingface_hub import login

# Replace with your real token
login(token="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")

# Stream Python files from The Stack (requires Hugging Face login)
dataset = load_dataset("bigcode/the-stack", data_dir="data/python", split="train", streaming=True)

# Collect a small subset for fine-tuning
processed = []
for i, ex in enumerate(dataset):
    code = ex["content"]
    if code and len(code) > 100 and "def" in code:
        processed.append({
            "prompt": "### Python code snippet",
            "completion": "\n" + code.strip()
        })
    if i >= 2000:  # adjust size as needed
        break

# Save to JSONL format
with open("/content/python_subset.jsonl", "w") as f:
    for item in processed:
        f.write(json.dumps(item) + "\n")

print(f"‚úÖ Saved {len(processed)} samples to /content/python_subset.jsonl")


# === 2. Validate your .jsonl dataset ===
def validate_jsonl(jsonl_path):
    valid = True
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f, 1):
            try:
                item = json.loads(line)
                assert 'prompt' in item and 'completion' in item
            except Exception as e:
                print(f"‚ùå Line {i} invalid: {e}")
                valid = False
    return valid

data_path = "/content/python_subset.jsonl"
if not validate_jsonl(data_path):
    raise ValueError("üõë Dataset validation failed. Fix the errors above.")

# === 3. Load & tokenize dataset ===
def load_local_dataset(jsonl_path):
    samples = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            ex = json.loads(line)
            samples.append({"text": ex['prompt'] + "\n" + ex['completion']})
    return Dataset.from_list(samples)

dataset = load_local_dataset(data_path)

tokenizer = AutoTokenizer.from_pretrained("Salesforce/codegen-350M-mono")
model     = AutoModelForCausalLM.from_pretrained("Salesforce/codegen-350M-mono")

# Reduce memory by activating gradient checkpointing
model.gradient_checkpointing_enable()

# Use EOS token as pad
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(example):
    return tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Split into 90% train, 10% eval
split         = tokenized_dataset.train_test_split(test_size=0.1)
train_dataset = split["train"]
eval_dataset  = split["test"]

# === 4. Set up TrainingArguments (legacy flags) ===
training_args = TrainingArguments(
    output_dir="/content/codegen_finetuned_cpu",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,

    # Force CPU
    no_cuda=True,

    # Legacy evaluation & saving flags (must match)
    do_eval=True,
    # Evaluation and saving strategies must match
    eval_strategy="steps",  # Evaluate every eval_steps
    eval_steps=500,
    save_strategy="steps",  # Save every save_steps
    save_steps=500,

    # Early stopping / best-checkpoint logic
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    logging_dir="/content/logs",
    logging_steps=100,
    save_total_limit=2,
    report_to="none",
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    loss_fct = torch.nn.CrossEntropyLoss()
    lgt = torch.tensor(logits)
    lbl = torch.tensor(labels)
    shift_logits = lgt[..., :-1, :].contiguous()
    shift_labels = lbl[..., 1:].contiguous()
    loss = loss_fct(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1)
    )
    return {
        "perplexity": math.exp(loss.item()),
        "eval_loss": loss.item()
    }

# === 5. Initialize Trainer & train ===
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
)

trainer.train()

# === 6. Save model & tokenizer ===
trainer.save_model("/content/codegen_finetuned_cpu")
tokenizer.save_pretrained("/content/codegen_finetuned_cpu")

print("\n‚úÖ CPU Training complete. Model saved to /content/codegen_finetuned_cpu")

from transformers import AutoTokenizer, AutoModelForCausalLM

def generate_code(prompt, model_path="/content/codegen_finetuned"):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(inputs["input_ids"], max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.95)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example usage:
prompt = "def fibonacci(n):"
print(generate_code(prompt))